from qlib.contrib.data.handler import Alpha158
from qlib.data.dataset import DatasetH
from qlib.data.dataset.handler import DataHandlerLP
from qlib.data import D
import pandas as pd
import numpy as np
import qlib
import os
import joblib
import lightgbm as lgb
from typing import Tuple
import optuna
from optuna.samplers import TPESampler

label = ["Ref($close, -5)/$close - 1"]
qlib_url = "C:\\Users\\22363\\.qlib\\qlib_data\\cn_data"
instruments_para = "csi800"  # å»ºè®®å…ˆç”¨ csi300 è·‘é€šï¼Œå†æ¢ csi800
MODEL_PATH = "results/lgb_model_CSI800_ltr.pkl"
start_date = "2020-01-01"
end_date = "2025-09-17"
predict_date = "2025-09-18"
region = "cn"


# -------- å·¥å…·å‡½æ•° --------
def get_prev_trading_date(predict_date: str) -> str:
    calendar = D.calendar(end_time=predict_date, freq="day")
    if len(calendar) < 2:
        raise ValueError(f"âŒ æ— æ³•è·å– {predict_date} çš„å‰ä¸€ä¸ªäº¤æ˜“æ—¥")
    if pd.Timestamp(predict_date) == calendar[-1]:
        return calendar[-2]
    return calendar[-1]


def _safe_rank_bins(y: pd.Series, bins: int = 5) -> pd.Series:
    """å•æ—¥æ¨ªæˆªé¢æ”¶ç›Šåˆ†æ¡¶ï¼Œè½¬æˆæ•´æ•°æ ‡ç­¾"""
    y = y.fillna(0)
    if len(y.unique()) < bins:
        return pd.Series([bins // 2] * len(y), index=y.index, dtype="int32")
    ranked = y.rank(method="first")
    labels = pd.qcut(ranked, bins, labels=False, duplicates="drop")
    return labels.fillna(bins // 2).astype("int32")


def prepare_data(dataset: DatasetH, segment: Tuple[str, str], bins: int = 5):
    """åƒ Qlib å†…éƒ¨ä¸€æ ·ï¼ŒæŒ‰æ—¥æœŸ prepareï¼Œé¿å…ä¸€æ¬¡æ€§å±•å¼€"""
    df = dataset.prepare(segment, col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)

    X_list, y_list, g_list = [], [], []
    for day, df_day in df.groupby(level=0):
        X_day = df_day["feature"].astype("float32").values
        y_raw = df_day["label"]
        if isinstance(y_raw, pd.DataFrame):
            y_raw = y_raw.iloc[:, 0]
        y_day = _safe_rank_bins(y_raw, bins=bins).values.astype("int32")

        if len(y_day) == 0:
            continue
        X_list.append(X_day)
        y_list.append(y_day)
        g_list.append(len(y_day))

    X = np.vstack(X_list).astype("float32")
    y = np.concatenate(y_list).astype("int32")
    groups = np.array(g_list, dtype="int32")
    return X, y, groups


# -------- è®­ç»ƒ / é¢„æµ‹ --------
# def train_model(model_path: str):
#     handler = Alpha158(
#         start_time=start_date,
#         end_time=end_date,
#         instruments=instruments_para,
#         fit_start_time=start_date,
#         fit_end_time=end_date,
#         label=label
#     )
#     handler.fetch()
#
#     dataset = DatasetH(handler=handler, segments={
#         "train": ("2020-01-01", "2024-12-31"),
#         "valid": ("2025-01-01", "2025-06-30"),
#     })
#
#     print("[prepare] è®­ç»ƒé›†...")
#     X_train, y_train, groups_train = prepare_data(dataset, "train", bins=5)
#     print("[prepare] éªŒè¯é›†...")
#     X_valid, y_valid, groups_valid = prepare_data(dataset, "valid", bins=5)
#
#     print("è®­ç»ƒé›†æ ·æœ¬:", X_train.shape, "groupsæ€»å’Œ:", groups_train.sum())
#     print("éªŒè¯é›†æ ·æœ¬:", X_valid.shape, "groupsæ€»å’Œ:", groups_valid.sum())
#
#     model = lgb.LGBMRanker(
#         objective="lambdarank",
#         metric="ndcg",
#         eval_at=[10, 20, 50],
#         max_depth=7,
#         num_leaves=127,
#         n_estimators=2000,
#         subsample=0.8,
#         colsample_bytree=0.8,
#         learning_rate=0.03,
#         reg_alpha=5.0,
#         reg_lambda=10.0,
#         n_jobs=8,
#         device="cpu"  # å…ˆ CPUï¼Œè·‘é€šåå†æ”¹ "gpu"
#     )
#
#     model.fit(
#         X_train, y_train,
#         group=groups_train,
#         eval_set=[(X_valid, y_valid)],
#         eval_group=[groups_valid],
#         callbacks=[
#             lgb.early_stopping(50),
#             lgb.log_evaluation(period=100)
#         ]
#     )
#
#     os.makedirs(os.path.dirname(model_path), exist_ok=True)
#     if os.path.exists(model_path):
#         os.remove(model_path)
#     joblib.dump(model, model_path)
#     print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
#     return model
def train_model(model_path: str):
    handler = Alpha158(
        start_time=start_date,
        end_time=end_date,
        instruments=instruments_para,
        fit_start_time=start_date,
        fit_end_time=end_date,
        label=label
    )
    handler.fetch()

    dataset = DatasetH(handler=handler, segments={
        "train": ("2020-01-01", "2024-12-31"),
        "valid": ("2025-01-01", "2025-06-30"),
    })

    print("[prepare] è®­ç»ƒé›†...")
    X_train, y_train, groups_train = prepare_data(dataset, "train", bins=5)
    print("[prepare] éªŒè¯é›†...")
    X_valid, y_valid, groups_valid = prepare_data(dataset, "valid", bins=5)

    def objective(trial):
        params = {
            "objective": "lambdarank",
            "metric": "ndcg",
            "eval_at": [10],
            "boosting_type": "gbdt",
            "num_leaves": trial.suggest_int("num_leaves", 31, 255),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
            "n_estimators": 2000,
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 10.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),
            "n_jobs": 8,
            "device": "cpu",  # å»ºè®®å…ˆ CPUï¼Œç¨³å®šåå†æ”¹ "gpu"
        }

        model = lgb.LGBMRanker(**params)
        model.fit(
            X_train, y_train,
            group=groups_train,
            eval_set=[(X_valid, y_valid)],
            eval_group=[groups_valid],
            callbacks=[
                lgb.early_stopping(50, verbose=False)
            ]
        )
        # Optuna æœ€å¤§åŒ–éªŒè¯é›† NDCG@10
        score = model.best_score_["valid_0"]["ndcg@10"]
        return score

    # Optuna æœç´¢
    study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
    study.optimize(objective, n_trials=30)  # å¯æ”¹ä¸ºæ›´å¤§ï¼Œä¾‹å¦‚ 100

    print("âœ… æœ€ä¼˜å‚æ•°:", study.best_params)
    print("âœ… æœ€ä¼˜ NDCG@10:", study.best_value)

    # ç”¨æœ€ä¼˜å‚æ•°é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹
    best_params = study.best_params
    best_params.update({
        "objective": "lambdarank",
        "metric": "ndcg",
        "eval_at": [10, 20, 50],
        "n_estimators": 2000,
        "n_jobs": 8,
        "device": "cpu"
    })

    model = lgb.LGBMRanker(**best_params)
    model.fit(
        X_train, y_train,
        group=groups_train,
        eval_set=[(X_valid, y_valid)],
        eval_group=[groups_valid],
        callbacks=[
            lgb.early_stopping(50),
            lgb.log_evaluation(period=100)
        ]
    )

    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    if os.path.exists(model_path):
        os.remove(model_path)
    joblib.dump(model, model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    return model


def load_model(model_path: str):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{model_path}")
    return joblib.load(model_path)


def predict_positions_for_date(predict_date: str, topk: int, model_path: str):
    feature_date = get_prev_trading_date(predict_date)

    handler = Alpha158(
        start_time=feature_date,
        end_time=feature_date,
        instruments=instruments_para,
        fit_start_time=start_date,
        fit_end_time=end_date,
        label=label
    )
    handler.fetch()

    ds = DatasetH(handler=handler, segments={"infer": (feature_date, feature_date)})
    df_infer = ds.prepare("infer", col_set="feature", data_key=DataHandlerLP.DK_I)
    X_infer = df_infer.astype("float32").values

    model = load_model(model_path)
    scores = model.predict(X_infer)

    pred_signal = pd.Series(scores, index=df_infer.index)
    top_stocks = pred_signal.sort_values(ascending=False).head(topk)
    weight = 1.0 / len(top_stocks)
    df_result = pd.DataFrame({
        "stock": [idx[-1] for idx in top_stocks.index],
        "weight": weight,
        "signal": top_stocks.values,
        "buy_date": predict_date
    }).reset_index(drop=True)

    return df_result, pred_signal


# ====== ç²˜è´´åˆ°ä½ çš„è„šæœ¬ä¸­ï¼ˆimports å·²æœ‰å³å¯ï¼‰ ======

def evaluate_on_valid(dataset: DatasetH, model, topk: int = 20, n_bins: int = 10):
    """
    åœ¨ 'valid' åˆ†æ®µä¸Šè¯„ä¼°ï¼šRankICã€TopK å‘½ä¸­ç‡ã€Top-Bottom ä»·å·®ã€å‡å€¼TopKæœªæ¥æ”¶ç›Š
    æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯â€œåŸå§‹è¿ç»­æ ‡ç­¾â€ï¼ˆæœªæ¥5æ—¥æ”¶ç›Šï¼‰ï¼Œä¸æ˜¯ç¦»æ•£åˆ†æ¡¶åçš„ yã€‚
    """
    # å–éªŒè¯é›†çš„åŸå§‹ç‰¹å¾ä¸æ ‡ç­¾
    df_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
    X_valid = df_valid["feature"].astype("float32").values

    y_valid = df_valid["label"]
    if isinstance(y_valid, pd.DataFrame):
        y_valid = y_valid.iloc[:, 0]  # å–ç¬¬ä¸€åˆ—

    # é¢„æµ‹æ‰“åˆ†
    pred = model.predict(X_valid)

    # ç»“æœè¡¨ï¼ŒMultiIndex: (date, instrument)
    res = pd.DataFrame({"pred": pred, "ret": y_valid.values}, index=df_valid.index)

    # â€”â€” æ—¥åº¦ RankICï¼ˆSpearmanï¼‰ï¼šcorr(rank(pred), rank(ret))ï¼Œç”¨ pandas æ’å + çš®å°”é€Š
    def _rank_ic_one_day(d: pd.DataFrame) -> float:
        rp = d["pred"].rank(method="first")
        rr = d["ret"].rank(method="first")
        return rp.corr(rr)  # çš®å°”é€Šç›¸å…³ï¼Œä½†è¾“å…¥æ˜¯ç§© â†’ ç­‰ä»·äº Spearman

    ic_daily = res.groupby(level=0).apply(_rank_ic_one_day).dropna()
    rank_ic_mean = float(ic_daily.mean())
    rank_ic_std = float(ic_daily.std(ddof=1)) if len(ic_daily) > 1 else float("nan")
    rank_ic_ir = (rank_ic_mean / rank_ic_std) if pd.notna(rank_ic_std) and rank_ic_std > 0 else float("nan")

    # â€”â€” TopKï¼šæ¯å¤©é€‰åˆ†æ•°æœ€é«˜çš„ K åªï¼Œç»Ÿè®¡å¹³å‡æœªæ¥æ”¶ç›Š & æ­£æ”¶ç›Šå‘½ä¸­ç‡
    def _topk_stats_one_day(d: pd.DataFrame) -> pd.Series:
        s = d.sort_values("pred", ascending=False).head(topk)["ret"]
        return pd.Series({"avg_ret": s.mean(), "hit_rate": (s > 0).mean()})

    topk_day = res.groupby(level=0).apply(_topk_stats_one_day)
    avg_topk_ret = float(topk_day["avg_ret"].mean())
    avg_topk_hit = float(topk_day["hit_rate"].mean())

    # â€”â€” å¤šç©ºä»·å·®ï¼šæŒ‰é¢„æµ‹åˆ†æ•°åˆ†ä½ï¼ˆn_binsï¼‰ï¼ŒTop åˆ†ä½å‡å€¼ - Bottom åˆ†ä½å‡å€¼
    def _decile_spread_one_day(d: pd.DataFrame, q: int = n_bins) -> float:
        # å¯¹åŒæ—¥æ ·æœ¬çš„â€œé¢„æµ‹ç§©â€å†åš q åˆ†ä½
        ranks = d["pred"].rank(method="first")
        try:
            buckets = pd.qcut(ranks, q, labels=False, duplicates="drop")
        except ValueError:
            # å½“å¤©æ ·æœ¬å¤ªå°‘æˆ–é‡å¤å¤ªå¤šï¼Œé€€åŒ–ä¸ºå…¨éƒ¨æ”¾ä¸­é—´ï¼Œä»·å·®è®°ä¸º 0
            return 0.0
        d = d.assign(bucket=buckets)
        top_mean = d.loc[d["bucket"] == d["bucket"].max(), "ret"].mean()
        bot_mean = d.loc[d["bucket"] == d["bucket"].min(), "ret"].mean()
        return float(top_mean - bot_mean)

    spread_daily = res.groupby(level=0).apply(_decile_spread_one_day, q=n_bins)
    long_short_spread = float(spread_daily.mean())

    # æ±‡æ€»æ‰“å°
    print("\n==== Validation Metrics ====")
    print(f"RankIC (mean): {rank_ic_mean:.4f} | RankIC (std): {rank_ic_std:.4f} | IR: {rank_ic_ir:.3f}")
    print(f"Top{topk} avg future ret (per-day mean): {avg_topk_ret:.4%}")
    print(f"Top{topk} hit-rate (ret>0): {avg_topk_hit:.2%}")
    print(f"Top-{n_bins} vs Bottom-{n_bins} spread (per-day mean): {long_short_spread:.4%}")
    print("ï¼ˆLightGBM è®­ç»ƒæ—¥å¿—é‡Œæ˜¾ç¤ºçš„ valid NDCG@K å³ä¸ºæ’åºè´¨é‡çš„å¦ä¸€è§†è§’ï¼‰\n")

    return {
        "rank_ic_mean": rank_ic_mean,
        "rank_ic_std": rank_ic_std,
        "rank_ic_ir": rank_ic_ir,
        "avg_topk_ret": avg_topk_ret,
        "avg_topk_hit": avg_topk_hit,
        "long_short_spread": long_short_spread,
        "ic_daily": ic_daily,
        "topk_daily": topk_day,
        "spread_daily": spread_daily,
    }


def market_hit_rate(dataset: DatasetH, segment: str = "valid") -> float:
    """
    è®¡ç®—éªŒè¯é›†æœŸé—´ï¼Œå¸‚åœºæ•´ä½“ä¸Šæ¶¨æ¦‚ç‡ï¼ˆè‚¡ç¥¨æœªæ¥æ”¶ç›Š > 0 çš„æ¯”ä¾‹ï¼‰
    """
    df = dataset.prepare(segment, col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)

    y = df["label"]
    if isinstance(y, pd.DataFrame):
        y = y.iloc[:, 0]  # å–ç¬¬ä¸€åˆ—

    hit_rate = (y > 0).mean()  # ä¸Šæ¶¨è‚¡ç¥¨å æ¯”
    print(f"ğŸ“Š {segment} æ•´ä½“å¸‚åœºä¸Šæ¶¨æ¦‚ç‡: {hit_rate:.2%}")
    return float(hit_rate)


# -------- å…¥å£ --------
if __name__ == "__main__":
    qlib.init(provider_uri=qlib_url, region=region)
    # âœ… è®­ç»ƒæ¨¡å‹ä¸€æ¬¡ï¼ˆå¦‚å·²è®­ç»ƒå¯è·³è¿‡ï¼‰åŠ è½½å·²è®­ç»ƒæ¨¡å‹
    # model = load_model(MODEL_PATH)
    # ===== 1. è®­ç»ƒæ¨¡å‹ =====
    model = train_model(model_path=MODEL_PATH)

    # ===== 2. æ„é€ å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„æ•°æ®é›†ï¼ˆç”¨æ¥è¯„ä¼°ï¼‰=====
    handler = Alpha158(
        start_time=start_date,
        end_time=end_date,
        instruments=instruments_para,
        fit_start_time=start_date,
        fit_end_time=end_date,
        label=label
    )
    handler.fetch()

    dataset = DatasetH(handler=handler, segments={
        "train": ("2020-01-01", "2025-06-01"),
        "valid": ("2025-01-01", "2025-09-17"),
    })

    # ===== 3. åœ¨éªŒè¯é›†ä¸Šåšè¯„ä¼° =====
    metrics = evaluate_on_valid(dataset, model, topk=10, n_bins=10)
    # ===== 4. å¯¹æ¯”å¸‚åœºåŸºå‡† =====
    market_hr = market_hit_rate(dataset, "valid")
    print(f"ç­–ç•¥ Top10 å‘½ä¸­ç‡: {metrics['avg_topk_hit']:.2%} | å¸‚åœºåŸºå‡†: {market_hr:.2%}")

    df_pos, pred_signal = predict_positions_for_date(predict_date, topk=10, model_path=MODEL_PATH)
    print(df_pos.head(20))
